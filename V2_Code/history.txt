Found cached dataset parquet (/home/ik3g20/.cache/huggingface/datasets/naver-clova-ix___parquet/naver-clova-ix--cord-v2-c97f979311033a44/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
/data/ik3g20/yann_ip/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/data/ik3g20/yann_ip/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
===> Found Device:  cuda
/data/ik3g20/yann_ip/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/data/ik3g20/yann_ip/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
===> Model Defined
Starting Training
Traceback (most recent call last):
  File "train.py", line 55, in <module>
    all_losses = train_model(model, train_loader, config['EPOCHS'], config['L_RATE'])
  File "train.py", line 32, in train_model
    loss = model(img_batch, bbox_batch, conf_batch)
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 171, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 181, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 89, in parallel_apply
    output.reraise()
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/_utils.py", line 543, in reraise
    raise exception
torch.cuda.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 64, in _worker
    output = module(*input, **kwargs)
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ik3g20/Multi-modal-Information-extraction-on-product-images-from-auction-websites/V2_Code/Model/one_stage_detector.py", line 27, in forward
    total_rpn_loss, feature_maps, proposals, positive_anc_ind_sep, GT_class_pos = self.rpn(images, bboxes, conf_scores)
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ik3g20/Multi-modal-Information-extraction-on-product-images-from-auction-websites/V2_Code/Model/rpn.py", line 79, in forward
    feature_maps = self.backbone(imgs)
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ik3g20/Multi-modal-Information-extraction-on-product-images-from-auction-websites/V2_Code/Model/Backbone_VGG16.py", line 21, in forward
    return self.bbNet(img)
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/data/ik3g20/yann_ip/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.92 GiB total capacity; 9.50 GiB already allocated; 495.44 MiB free; 9.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

